# Cognifast AI - Backend

**Backend API and WebSocket server for Cognifast AI learning platform**

The backend provides RESTful API endpoints for document management and chat functionality, along with real-time WebSocket streaming for AI responses using LangGraph and OpenAI.

## üéØ Overview

The Cognifast backend is built with Node.js, Express, and TypeScript, providing:

- **Document Management**: Upload, process, and store documents with vector embeddings
- **AI Chat System**: Intelligent document-grounded conversations with streaming responses
- **Real-time Communication**: WebSocket-based token-by-token streaming
- **LangChain**: AI framework for building LLM applications with document processing and embeddings
- **LangGraph Orchestration**: Multi-agent workflow for routing, retrieval, generation, and quality assurance

## ‚ú® Features

### üìÑ Document Management
- Upload documents (PDF, DOC, DOCX, TXT)
- Automatic text extraction and chunking
- Vector embedding generation
- Storage in Supabase with PgVector for semantic search

### üí¨ AI Chat System
- **Multi-Agent Architecture**: Router, Retrieval, Generator, and Quality agents
- **Intelligent Routing**: Automatically determines if document retrieval is needed
- **Context-Aware Responses**: Uses retrieved document chunks for grounded answers
- **Quality Assurance**: Automatic response quality evaluation and retry mechanism
- **Streaming Responses**: Real-time token-by-token streaming via WebSocket

### üîå Real-time Communication
- Socket.io WebSocket server
- Token-by-token message streaming
- Loading stage updates (router, retrieval, generator)
- Conversation room management

## üõ† Tech Stack

### Core
- **Node.js** 18+ with TypeScript
- **Express.js** - RESTful API framework
- **Socket.io** - WebSocket server for real-time communication

### AI/ML
- **LangChain** - AI framework for building LLM applications
- **LangGraph** - State graph for orchestrating multi-agent workflows
- **OpenAI API** - GPT-4o-mini for routing, GPT-4o for generation
- **OpenAI Embeddings** - text-embedding-3-small for vector embeddings

### Database & Storage
- **Supabase** - PostgreSQL database with PgVector extension
- **Supabase Storage** - File storage for uploaded documents

### Utilities
- **Multer** - File upload handling
- **pdf-parse** - PDF text extraction
- **mammoth** - DOCX text extraction
- **uuid** - Unique ID generation
- **helmet** - Security headers
- **cors** - Cross-origin resource sharing

## üìÅ Project Structure

```
backend/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ agents/              # LangGraph agents
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ chat/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ router.agent.ts      # Routes queries (retrieve/direct/clarify)
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ retrieval.agent.ts   # Retrieves relevant document chunks
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ generator.agent.ts   # Generates AI responses (streaming)
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ quality.agent.ts     # Evaluates response quality
‚îÇ   ‚îú‚îÄ‚îÄ controllers/         # Request handlers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat.controller.ts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ document.controller.ts
‚îÇ   ‚îú‚îÄ‚îÄ db/                  # Database configuration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dbConnection.ts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schema.sql
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ migrations/
‚îÇ   ‚îú‚îÄ‚îÄ graphs/              # LangGraph state graphs
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ chat.graph.ts    # Chat workflow orchestration
‚îÇ   ‚îú‚îÄ‚îÄ middleware/          # Express middleware
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ upload.middleware.ts
‚îÇ   ‚îú‚îÄ‚îÄ routes/              # API route definitions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat.routes.ts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ document.routes.ts
‚îÇ   ‚îú‚îÄ‚îÄ services/            # Business logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat.service.ts           # Conversation management
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat-stream.service.ts    # WebSocket streaming orchestration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ document.service.ts       # Document processing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embedding.service.ts      # Embedding generation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ retrieval.service.ts      # Vector search
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ storage.service.ts        # File storage
‚îÇ   ‚îú‚îÄ‚îÄ sockets/             # WebSocket handlers
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ chat.socket.ts   # Socket.io event handlers
‚îÇ   ‚îú‚îÄ‚îÄ types/               # TypeScript type definitions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat.types.ts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ document.types.ts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ quiz.types.ts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ summary.types.ts
‚îÇ   ‚îú‚îÄ‚îÄ utils/               # Utility functions
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ logger.ts        # Logging utility
‚îÇ   ‚îî‚îÄ‚îÄ index.ts             # Application entry point
‚îú‚îÄ‚îÄ dist/                    # Compiled JavaScript (generated)
‚îú‚îÄ‚îÄ uploads/                 # Temporary file storage
‚îú‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ tsconfig.json
‚îî‚îÄ‚îÄ .env                     # Environment variables (not in repo)
```

## üöÄ Getting Started

### Prerequisites

- **Node.js** 18+ and npm
- **Supabase Account** - PostgreSQL database with PgVector extension
- **OpenAI API Key** - For LLM and embeddings

### Installation

1. **Install dependencies**
   ```bash
   npm install
   ```

2. **Configure environment variables**
   
   Create a `.env` file in the `backend` directory:
   ```env
   # Server
   PORT=3000
   NODE_ENV=development
   FRONTEND_URL=http://localhost:5173

   # Supabase
   SUPABASE_URL=your-supabase-project-url
   SUPABASE_KEY=your-supabase-service-role-key

   # OpenAI
   OPENAI_API_KEY=your-openai-api-key
   ```

3. **Set up database**
   
   Run the database schema and migrations:
   ```bash
   # Connect to your Supabase database and run:
   # - src/db/schema.sql (creates tables)
   # - src/db/vector_search_function.sql (creates vector search function)
   ```

4. **Start development server**
   ```bash
   npm run dev
   ```

   The server will start on `http://localhost:3000`

### Available Scripts

- `npm run dev` - Start development server with hot reload (nodemon)
- `npm run build` - Compile TypeScript to JavaScript
- `npm start` - Start production server (requires build first)

## üîß Environment Variables

| Variable | Description | Required | Default |
|----------|-------------|----------|---------|
| `PORT` | Server port | No | `3000` |
| `NODE_ENV` | Environment (development/production) | No | `development` |
| `FRONTEND_URL` | Frontend URL for CORS | No | `http://localhost:5173` |
| `SUPABASE_URL` | Supabase project URL | Yes | - |
| `SUPABASE_KEY` | Supabase service role key | Yes | - |
| `OPENAI_API_KEY` | OpenAI API key | Yes | - |

## üì° API Endpoints

### Document Management

#### Upload Document
```http
POST /api/documents/upload
Content-Type: multipart/form-data

Body: { document: File }
```

**Response:**
```json
{
  "id": "uuid",
  "originalName": "document.pdf",
  "storedName": "uuid.pdf",
  "mimeType": "application/pdf",
  "size": 12345,
  "uploadedAt": "2024-01-01T00:00:00.000Z"
}
```

#### Get All Documents
```http
GET /api/documents
```

**Response:**
```json
[
  {
    "id": "uuid",
    "originalName": "document.pdf",
    "uploadedAt": "2024-01-01T00:00:00.000Z"
  }
]
```

#### Get Document by ID
```http
GET /api/documents/:id
```

### Chat Management

#### Start Conversation
```http
POST /api/chat/conversations
Content-Type: application/json

{
  "documentIds": ["uuid1", "uuid2"]
}
```

**Response:**
```json
{
  "id": "conversation-uuid",
  "documentIds": ["uuid1", "uuid2"],
  "createdAt": "2024-01-01T00:00:00.000Z",
  "updatedAt": "2024-01-01T00:00:00.000Z"
}
```

#### Get All Conversations
```http
GET /api/chat/conversations
```

#### Get Conversation
```http
GET /api/chat/conversations/:conversationId
```

**Response:**
```json
{
  "conversation": {
    "id": "uuid",
    "documentIds": ["uuid1"],
    "createdAt": "2024-01-01T00:00:00.000Z",
    "updatedAt": "2024-01-01T00:00:00.000Z"
  },
  "messages": [
    {
      "id": "uuid",
      "conversationId": "uuid",
      "role": "user",
      "content": "Hello",
      "createdAt": "2024-01-01T00:00:00.000Z"
    }
  ]
}
```

#### Send Message (REST - Non-streaming)
```http
POST /api/chat/conversations/:conversationId/messages
Content-Type: application/json

{
  "message": "What is this document about?"
}
```

**Response:**
```json
{
  "message": {
    "id": "uuid",
    "conversationId": "uuid",
    "role": "assistant",
    "content": "This document is about...",
    "sources": [
      {
        "chunkId": "uuid",
        "documentId": "uuid",
        "documentName": "document.pdf",
        "similarity": 0.95
      }
    ],
    "createdAt": "2024-01-01T00:00:00.000Z"
  }
}
```

#### Delete Conversation
```http
DELETE /api/chat/conversations/:conversationId
```

## üîå WebSocket Events

### Client ‚Üí Server

#### Join Conversation
```javascript
socket.emit('join_conversation', {
  conversationId: 'uuid'
});
```

#### Send Message
```javascript
socket.emit('send_message', {
  conversationId: 'uuid',
  message: 'What is this document about?'
});
```

#### Leave Conversation
```javascript
socket.emit('leave_conversation', {
  conversationId: 'uuid'
});
```

### Server ‚Üí Client

#### Joined Confirmation
```javascript
socket.on('joined_conversation', (data) => {
  // data: { conversationId: 'uuid' }
});
```

#### Message Start
```javascript
socket.on('message_start', (data) => {
  // data: { conversationId: 'uuid' }
  // Streaming is about to begin
});
```

#### Loading Stage
```javascript
socket.on('loading_stage', (data) => {
  // data: {
  //   conversationId: 'uuid',
  //   stage: 'router' | 'retrieval' | 'generator',
  //   message: 'Looking for cues...' | 'Reviewing document...' | 'Generating response...'
  // }
});
```

#### Message Token (Streaming)
```javascript
socket.on('message_token', (data) => {
  // data: {
  //   conversationId: 'uuid',
  //   messageId: 'uuid',
  //   token: 'Hello'
  // }
  // Emitted for each token in the response
});
```

#### Message End
```javascript
socket.on('message_end', (data) => {
  // data: {
  //   conversationId: 'uuid',
  //   messageId: 'uuid',
  //   message: {
  //     id: 'uuid',
  //     role: 'assistant',
  //     content: 'Complete response...',
  //     sources: [...],
  //     createdAt: '2024-01-01T00:00:00.000Z'
  //   }
  // }
});
```

#### Error
```javascript
socket.on('error', (data) => {
  // data: {
  //   conversationId: 'uuid',
  //   message: 'Error message'
  // }
});
```

## ü§ñ Agents & Services

### Router Agent
**File:** `src/agents/chat/router.agent.ts`

Analyzes user queries and determines the routing strategy:
- **`retrieve`**: Query needs document context ‚Üí routes to Retrieval Agent
- **`direct_answer`**: Can answer without retrieval (greetings, thanks) ‚Üí routes directly to Generator
- **`clarify`**: Query is unclear ‚Üí routes to Generator for clarification

**Model:** GPT-4o-mini (fast, deterministic routing)

### Retrieval Agent
**File:** `src/agents/chat/retrieval.agent.ts`

Retrieves relevant document chunks using vector search:
- Calls `RetrievalService.retrieveRelevantChunks()`
- Returns top 5 most relevant chunks with similarity scores
- Only executed when Router Agent decides `retrieve`

### Generator Agent
**File:** `src/agents/chat/generator.agent.ts`

Generates AI responses with two modes:

1. **Streaming Mode** (when `onToken` callback exists):
   - Uses `llm.stream()` for token-by-token generation
   - Calls `onToken(token)` for each token
   - Provides real-time streaming to frontend

2. **Non-Streaming Mode** (REST API compatibility):
   - Uses `llm.invoke()` for complete response
   - Returns full message at once

**Model:** GPT-4o (high-quality responses)

**Prompt Types:**
- **RAG Prompt**: When retrieved chunks are available
- **Direct Answer Prompt**: For general queries
- **Clarification Prompt**: When query is unclear

### Quality Agent
**File:** `src/agents/chat/quality.agent.ts`

Evaluates response quality and triggers regeneration if needed:
- Returns `'good'` or `'poor'`
- If `'poor'` and `retryCount < 2`: routes back to Generator Agent
- If `'good'` or `retryCount >= 2`: ends workflow

**Model:** GPT-4o-mini (fast evaluation)

**Note:** Quality check is skipped for first message to improve response time.

### Chat Service
**File:** `src/services/chat.service.ts`

Manages conversations and messages:
- CRUD operations for conversations
- Message persistence
- Conversation-document relationships

### Chat Stream Service
**File:** `src/services/chat-stream.service.ts`

Orchestrates WebSocket streaming:
- Creates `onToken` callback for token-by-token streaming
- Streams LangGraph execution
- Emits WebSocket events (`message_start`, `loading_stage`, `message_token`, `message_end`)
- Saves final messages to database

### Retrieval Service
**File:** `src/services/retrieval.service.ts`

Performs vector similarity search:
- Generates query embeddings via `EmbeddingService`
- Calls Supabase `match_documents_chunks` RPC function
- Returns top K most relevant chunks with similarity scores
- Enriches chunks with document names

### Embedding Service
**File:** `src/services/embedding.service.ts`

Handles text embeddings:
- Generates embeddings using OpenAI `text-embedding-3-small`
- Chunks documents for processing
- Generates embeddings for document chunks and queries

### Document Service
**File:** `src/services/document.service.ts`

Manages document processing:
- Text extraction (PDF, DOCX, TXT)
- Document chunking
- Embedding generation and storage
- Document metadata management

## üóÑ Database Schema

### Tables

#### `documents`
- `id` (UUID, Primary Key)
- `original_name` (TEXT)
- `stored_name` (TEXT)
- `mime_type` (TEXT)
- `size` (BIGINT)
- `uploaded_at` (TIMESTAMP)

#### `document_chunks`
- `id` (UUID, Primary Key)
- `document_id` (UUID, Foreign Key ‚Üí documents)
- `chunk_index` (INTEGER)
- `content` (TEXT)
- `embedding` (VECTOR(1536)) - PgVector
- `created_at` (TIMESTAMP)

#### `conversations`
- `id` (UUID, Primary Key)
- `created_at` (TIMESTAMP)
- `updated_at` (TIMESTAMP)

#### `conversation_documents`
- `conversation_id` (UUID, Foreign Key ‚Üí conversations)
- `document_id` (UUID, Foreign Key ‚Üí documents)

#### `messages`
- `id` (UUID, Primary Key)
- `conversation_id` (UUID, Foreign Key ‚Üí conversations)
- `role` (TEXT) - 'user' | 'assistant'
- `content` (TEXT)
- `sources` (JSONB) - Array of retrieved chunk references
- `created_at` (TIMESTAMP)

### Vector Search Function

The `match_documents_chunks` RPC function performs cosine similarity search:
```sql
SELECT * FROM match_documents_chunks(
  query_embedding VECTOR(1536),
  match_count INT,
  filter_document_ids UUID[]
)
```

## üîÑ LangGraph Workflow

The chat workflow is orchestrated by LangGraph:

```
START
  ‚Üì
Router Agent (analyzes query)
  ‚Üì
  ‚îú‚îÄ‚Üí [retrieve] ‚Üí Retrieval Agent ‚Üí Generator Agent
  ‚îî‚îÄ‚Üí [direct_answer/clarify] ‚Üí Generator Agent
  ‚Üì
Quality Agent (evaluates response)
  ‚Üì
  ‚îú‚îÄ‚Üí [poor & retries < 2] ‚Üí Generator Agent (retry)
  ‚îî‚îÄ‚Üí [good or max retries] ‚Üí END
```

**State Graph:** `src/graphs/chat.graph.ts`

**State Structure:**
```typescript
{
  conversationId: string;
  documentIds: string[];
  messages: Message[];
  currentQuery: string;
  retrievedChunks: RetrievedChunk[];
  routerDecision: 'retrieve' | 'direct_answer' | 'clarify';
  responseQuality: 'good' | 'poor' | 'pending';
  retryCount: number;
  metadata: {
    startTime: number;
    isFirstMessage: boolean;
    onToken?: (token: string) => void; // For streaming
  };
}
```

## üß™ Development

### Running in Development Mode

```bash
npm run dev
```

Uses `nodemon` for automatic restart on file changes.

### Building for Production

```bash
npm run build
```

Compiles TypeScript to JavaScript in the `dist/` directory.

### Running in Production

```bash
npm start
```

Runs the compiled JavaScript from `dist/index.js`.

### Logging

The application uses a custom logger (`src/utils/logger.ts`):
- **Development**: Logs all levels (info, warn, error, debug)
- **Production**: Logs only errors

Log format: `[TIMESTAMP] [LEVEL] [CONTEXT] Message`

## üêõ Troubleshooting

### Database Connection Issues

**Error:** `Invalid API key or authentication failed`
- **Solution:** Check `SUPABASE_KEY` in `.env` file

**Error:** `Network error` or `ENOTFOUND`
- **Solution:** Check `SUPABASE_URL` in `.env` file

### WebSocket Connection Issues

**Error:** `CORS policy blocked`
- **Solution:** Ensure `FRONTEND_URL` in `.env` matches your frontend URL

**Error:** `Socket not connected`
- **Solution:** Check that Socket.io server is running and frontend is connecting to correct port

### OpenAI API Issues

**Error:** `Invalid API key`
- **Solution:** Verify `OPENAI_API_KEY` in `.env` file

**Error:** `Rate limit exceeded`
- **Solution:** Check OpenAI API usage limits and implement rate limiting if needed

### Vector Search Issues

**Error:** `No chunks found`
- **Solution:** 
  - Verify documents have been processed and chunks exist in `document_chunks` table
  - Check that `match_documents_chunks` RPC function exists in database
  - Verify embeddings are generated correctly

### File Upload Issues

**Error:** `File too large`
- **Solution:** Check Multer configuration in `src/middleware/upload.middleware.ts`

**Error:** `Unsupported file type`
- **Solution:** Verify file extension is supported (PDF, DOC, DOCX, TXT)

## üìö Additional Resources

- [LangChain Documentation](https://js.langchain.com/)
- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)
- [Socket.io Documentation](https://socket.io/docs/v4/)
- [Supabase Documentation](https://supabase.com/docs)
- [OpenAI API Documentation](https://platform.openai.com/docs)

## üìù License

ISC

## üë§ Author

Gideon Ayeni

